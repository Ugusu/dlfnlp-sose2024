epochs: 6
learning_rate: 3e-05
optimizer: SophiaG
optimizer_params: {'lr': 3e-05, 'betas': (0.1, 0.001), 'rho': 0.04, 'weight_decay': 0.1}
use_scheduler: True
scheduler_step_size: 1
scheduler_gamma: 0.675
batch_size: 1
max_length: 256
gradual_unfreezing: True
num_layers_to_freeze: 12
rl_weight: 0.85
dataset: etpc-paraphrase-train.csv
subset: 0.001
val_dataset: etpc-paraphrase-dev.csv
test_dataset: etpc-paraphrase-generation-test-student.csv
penalized_bleu_epochs: [5.324725802765263, 5.210417666959644, 5.631790534089388, 5.3285073040239626, 5.884640988913841, 5.829334829711312]
penalized_bleu_val: 5.884640988913841
penalized_bleu_test: 0.0
prefix: True
prefix_length: 10
prefix_method: indirect
use_gpu: False
seed: 11711
model: facebook/bart-large
tokenizer: facebook/bart-large
input_format: {masked_sentence} {tokenizer.sep_token} {' '.join(sentence1_tags)}
target_format: {sentence2}
other_details: masked a random verb, adjective, noun, and conjunction 
rotated the sentence parts if there is , in betweenAdding the least frequent tokens between the two sentences to half of the training dataThis is to make sure the model learns to generate the most important tokens, and also learnthat htis type of input may be None, as it is for the validation and test
example_inputs: ['Through Thursday, Oracle said 34.75 million PeopleSoft shares had been tendered.']
example_references: ['Some 34.7 million shares have been tendered, Oracle said in a statement.']
example_predictions: ['As of Wednesday, more than 34.75 million PeopleSoft shares had been tendered. Through Thursday,']
